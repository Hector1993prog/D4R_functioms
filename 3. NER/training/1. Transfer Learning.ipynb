{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRansfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import torch # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate # type: ignore\n",
    "from transformers import( # type: ignore\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from TokenClassificationEncoderModels import(\n",
    "    RobertaTokenClassifier_With_GRU,\n",
    "    RobertaTokenClassifier_With_LSTM,\n",
    "    BertTokenClassifier_With_GRU,\n",
    "    BertTokenClassifier_With_LSTM \n",
    ") \n",
    "from utilities_ner_functions import(\n",
    "    tokenize_and_align_labels_black_box,\n",
    "    baseDataset,\n",
    ") \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(r'/home/tensorboard/Documentos/1. D4R/3. NER/training/dataset_NER_completo_23-05-24.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "df['sentence'] = df['sentence'].apply(ast.literal_eval)\n",
    "df['tags'] = df['tags'].apply(ast.literal_eval)\n",
    "df['encoded_tags'] = df['encoded_tags'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_ids = {\n",
    "    'O': 0,\n",
    "    'GOD': 1,\n",
    "    'JUS': 2,\n",
    "    'CHRI': 3,\n",
    "    'SACRA': 4,\n",
    "    'HERESY': 5,\n",
    "    'PER': 6,\n",
    "    'PLACE': 7,\n",
    "    'ORG': 8\n",
    "    }\n",
    "ids_to_labels = {v:k for k, v in labels_to_ids.items()}\n",
    "\n",
    "labels_list = [x for x in ids_to_labels.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepared = df[['sentence', 'encoded_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, eval_df = train_test_split(df_prepared, train_size= 0.8, random_state=42, shuffle=True)\n",
    "eval_df, test_df = train_test_split(eval_df, test_size=0.5, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained(\"/media/tensorboard/PRODUCCIÓN Y ACADÉMICO/TRABAJO/D4R_models/NER_models/General_models/roberta-base-bne-Linear-NER/checkpoint-50000\",  add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs_train = tokenize_and_align_labels_black_box(train_df, tokenizer,token_column='sentence', tag_column='encoded_tags')\n",
    "tokenized_inputs_eval = tokenize_and_align_labels_black_box(eval_df, tokenizer,token_column='sentence', tag_column='encoded_tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-base-bne and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_CHECKPOINT ='/media/tensorboard/PRODUCCIÓN Y ACADÉMICO/TRABAJO/D4R_models/NER_models/General_models/roberta-base-bne-Linear-NER/checkpoint-50000'\n",
    "\n",
    "#carga el modelo qye ya está ajustado\n",
    "old_model= AutoModelForTokenClassification.from_pretrained(MODEL_CHECKPOINT) #carga el modelo que ya está ajustado\n",
    "#carga un nuevo modelo (aqui se puede probar Bi-LSTM,RNN o GRU en configuraciones propias)\n",
    "new_model=AutoModelForTokenClassification.from_pretrained('PlanTL-GOB-ES/roberta-base-bne', num_labels= len(labels_list),id2label=ids_to_labels, label2id=labels_to_ids)\n",
    "\n",
    "#Solo usamos la configuración de pesos aprendida del modelo\n",
    "new_model.roberta =old_model.roberta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in new_model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Create a new linear layer for NER\n",
    "new_classifier = torch.nn.Linear(in_features=new_model.config.hidden_size, out_features=new_model.config.num_labels)\n",
    "\n",
    "# Replace the existing classifier with the new one\n",
    "new_model.classifier = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "batch_size = 16\n",
    "input_ids = torch.randint(0, new_model.config.vocab_size, (batch_size, tokenized_inputs_train['input_ids'][0].shape[0]))\n",
    "# 0, new_model.config.vocab_size, significa que creamos un tensor desde cero hasta el máximo del vocabulario del transformer\n",
    "# (batch_size, tokenized_inputs_train['input_ids'][0].shape[0]) for que adquiere, en este caso, el batch es 16 y el shape del input ID es 460\n",
    "\n",
    "\n",
    "summary(new_model,\n",
    "            input_data=input_ids,\n",
    "                       col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "                       )\n",
    "\n",
    "text_summary = str(summary(new_model,\n",
    "            input_data=input_ids,\n",
    "                       col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "                       ))\n",
    "\n",
    "with open(f'sumary of TF linear.txt', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(text_summary)\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train= baseDataset(tokenized_inputs_train)\n",
    "dataset_test =baseDataset(tokenized_inputs_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seqeval\n",
    "import numpy as np\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Computes the precision, recall, F1, and accuracy scores for the given predictions and labels.\n",
    "\n",
    "    Args:\n",
    "        p (tuple): A tuple containing the predictions and labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the precision, recall, F1, and accuracy scores.\n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [labels_list[p] for (p, l) in zip(prediction, label) if l!= -100] # type: ignore\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [labels_list[l] for (p, l) in zip(prediction, label) if l!= -100] # type: ignore\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"], # type: ignore\n",
    "        \"recall\": results[\"overall_recall\"], # type: ignore\n",
    "        \"f1\": results[\"overall_f1\"], # type: ignore\n",
    "        \"accuracy\": results[\"overall_accuracy\"], # type: ignore\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='roberta-base-bne-NER-TF-100-epochs-2-4e',\n",
    "    learning_rate=2e-6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps = 500\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=new_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhector-investigacion-sociocul\u001b[0m (\u001b[33mhlhdatascience\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tensorboard/Documentos/1. D4R/3. NER/training/wandb/run-20240524_113106-fq1xbu9p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hlhdatascience/huggingface/runs/fq1xbu9p' target=\"_blank\">desert-snow-130</a></strong> to <a href='https://wandb.ai/hlhdatascience/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hlhdatascience/huggingface' target=\"_blank\">https://wandb.ai/hlhdatascience/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hlhdatascience/huggingface/runs/fq1xbu9p' target=\"_blank\">https://wandb.ai/hlhdatascience/huggingface/runs/fq1xbu9p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdb62498c964945989a3ccb18d10a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8592, 'grad_norm': 10.82496166229248, 'learning_rate': 1.5633187772925764e-06, 'epoch': 2.18}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43804f5e66de4468a72fa28257c6a3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'labels_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/1. D4R/.venv/lib/python3.10/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/1. D4R/.venv/lib/python3.10/site-packages/transformers/trainer.py:2278\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/Documentos/1. D4R/.venv/lib/python3.10/site-packages/transformers/trainer.py:2662\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2660\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2662\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2663\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/Documentos/1. D4R/.venv/lib/python3.10/site-packages/transformers/trainer.py:3467\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3464\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3466\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3467\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3468\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3470\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3471\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3475\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3477\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/Documentos/1. D4R/.venv/lib/python3.10/site-packages/transformers/trainer.py:3719\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3715\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3716\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[1;32m   3717\u001b[0m         )\n\u001b[1;32m   3718\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3719\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3720\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3721\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Documentos/1. D4R/3. NER/training/utilities_ner_functions.py:86\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m     83\u001b[0m predictions, labels \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m     84\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m true_predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     87\u001b[0m     [labels_list[p] \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l\u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[1;32m     89\u001b[0m ]\n\u001b[1;32m     90\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     91\u001b[0m     [labels_list[l] \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l\u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[1;32m     93\u001b[0m ]\n\u001b[1;32m     95\u001b[0m results \u001b[38;5;241m=\u001b[39m seqeval\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mtrue_predictions, references\u001b[38;5;241m=\u001b[39mtrue_labels)\n",
      "File \u001b[0;32m~/Documentos/1. D4R/3. NER/training/utilities_ner_functions.py:87\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     83\u001b[0m predictions, labels \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m     84\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     86\u001b[0m true_predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 87\u001b[0m     [labels_list[p] \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l\u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[1;32m     89\u001b[0m ]\n\u001b[1;32m     90\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     91\u001b[0m     [labels_list[l] \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l\u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[1;32m     93\u001b[0m ]\n\u001b[1;32m     95\u001b[0m results \u001b[38;5;241m=\u001b[39m seqeval\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mtrue_predictions, references\u001b[38;5;241m=\u001b[39mtrue_labels)\n",
      "File \u001b[0;32m~/Documentos/1. D4R/3. NER/training/utilities_ner_functions.py:87\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     83\u001b[0m predictions, labels \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m     84\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     86\u001b[0m true_predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 87\u001b[0m     [\u001b[43mlabels_list\u001b[49m[p] \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l\u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[1;32m     89\u001b[0m ]\n\u001b[1;32m     90\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     91\u001b[0m     [labels_list[l] \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l\u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[1;32m     93\u001b[0m ]\n\u001b[1;32m     95\u001b[0m results \u001b[38;5;241m=\u001b[39m seqeval\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mtrue_predictions, references\u001b[38;5;241m=\u001b[39mtrue_labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels_list' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using costum models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TokenClassificationEncoderModels import RobertaTokenClassifier_With_GRU, RobertaTokenClassifier_With_LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= RobertaTokenizerFast.from_pretrained(\"hlhdatscience/roberta-base-bne-NER\",  add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(dataframe, tokenizer):\n",
    "    # Tokenize inputs using the provided tokenizer\n",
    "    tokenized_inputs = tokenizer(dataframe['tokens'].tolist(),\n",
    "                                 truncation=True,\n",
    "                                 is_split_into_words=True,\n",
    "                                 padding=True,\n",
    "                                 max_length=200,\n",
    "                                 return_tensors='pt'\n",
    "                                )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(dataframe['ner_tags'].tolist()):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                if word_idx < len(label):  # Check if the word index is within the range of the label list\n",
    "                    label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    tokenized_inputs[\"labels\"] = torch.tensor(labels)\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs_train = tokenize_and_align_labels(train_df, tokenizer=tokenizer)\n",
    "\n",
    "tokenized_inputs_test = tokenize_and_align_labels(test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carga el modelo que ya está ajustado\n",
    "old_model= RobertaForTokenClassification.from_pretrained(\"hlhdatscience/roberta-base-bne-NER\") #carga el modelo qye ya está ajustado\n",
    "#carga un nuevo modelo costumizado\n",
    "new_model=RobertaTokenClassifierWithLSTM.from_pretrained('PlanTL-GOB-ES/roberta-base-bne', num_labels= len(labels_list),id2label=ids_to_lables, label2id=labels_to_ids)\n",
    "\n",
    "#Solo usamos la configuración de pesos aprendida del modelo\n",
    "new_model.roberta =old_model.roberta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "#fronzen model weights\n",
    "for param in new_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Create the new layers\n",
    "new_leaky_relu = nn.LeakyReLU(negative_slope=0.01)  # Adjust the negative slope as needed\n",
    "\n",
    "\n",
    "# Linear layer for feature transformation\n",
    "new_dense_1 = nn.Linear(new_model.config.hidden_size, 512)\n",
    "new_dense_2 = nn.Linear(512, 256)\n",
    "# Bidirectional LSTM layer\n",
    "new_bidirectional = nn.LSTM(\n",
    "            input_size=512,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "# Dropout for LSTM regularization\n",
    "new_dropout_lstm = nn.Dropout(0.1)  # Adjust dropout rate as needed\n",
    "\n",
    "# Linear layer for classification\n",
    "new_classifier = nn.Linear(256, new_model.config.num_labels)\n",
    "\n",
    "# Replace the existing frozen layers for the unfrozen ones\n",
    "\n",
    "new_model.leaky_relu = new_leaky_relu\n",
    "new_model.dense_1 = new_dense_1\n",
    "new_model.bidirectional = new_bidirectional\n",
    "new_model.dropout_lstm = new_dropout_lstm\n",
    "new_model.dense_2 = new_dense_2\n",
    "new_model.classifier = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [labels_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [labels_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train= baseDataset(tokenized_inputs_train)\n",
    "dataset_test =baseDataset(tokenized_inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='roberta-base-bne-NER-TF-Bi-LSTM-100-epochs-2-4e',\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=new_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carga el modelo que ya está ajustado\n",
    "old_model= RobertaForTokenClassification.from_pretrained(\"hlhdatscience/roberta-base-bne-NER\") #carga el modelo qye ya está ajustado\n",
    "#carga un nuevo modelo costumizado\n",
    "new_model=RobertaTokenClassifierWithGRU.from_pretrained('PlanTL-GOB-ES/roberta-base-bne', num_labels= len(labels_list),id2label=ids_to_lables, label2id=labels_to_ids)\n",
    "\n",
    "#Solo usamos la configuración de pesos aprendida del modelo\n",
    "new_model.roberta =old_model.roberta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "#fronzen model weights\n",
    "for param in new_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Create the new layers\n",
    "\n",
    "\n",
    "new_leaky_relu = nn.LeakyReLU(negative_slope=0.01)  # Adjust the negative slope as needed\n",
    "\n",
    "        # Linear layer for feature transformation\n",
    "new_dense_1 = nn.Linear(new_model.config.hidden_size, 512)\n",
    "\n",
    "        # Bidirectional GRU layer\n",
    "new_bidirectional = nn.GRU(\n",
    "            input_size=512,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Dropout for GRU regularization\n",
    "new_dropout_GRU = nn.Dropout(0.1)  # Adjust dropout rate as needed\n",
    "\n",
    "        # Second Linear layer for feature transformation\n",
    "new_dense_2 = nn.Linear(512, 256)\n",
    "\n",
    "        # Linear layer for classification\n",
    "new_classifier = nn.Linear(256, new_model.config.num_labels)\n",
    "\n",
    "\n",
    "# Replace the existing frozen layers for the unfrozen ones\n",
    "\n",
    "new_model.leaky_relu = new_leaky_relu\n",
    "new_model.dense_1 = new_dense_1\n",
    "new_model.bidirectional = new_bidirectional\n",
    "new_model.dropout_GRU = new_dropout_GRU\n",
    "new_model.dense_2 = new_dense_2\n",
    "new_model.classifier = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [labels_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [labels_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train= baseDataset(tokenized_inputs_train)\n",
    "dataset_test =baseDataset(tokenized_inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='roberta-base-bne-NER-TF-Bi-GRU-100-epochs-2-4e',\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=new_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important link to understand val loss going up and rest of the val metricsa as well (positive correlation instead of the inverse one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/282160/how-is-it-possible-that-validation-loss-is-increasing-while-validation-accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
