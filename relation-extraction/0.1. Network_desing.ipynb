{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing the Architecture for Relation Extraction as sequence classification problem\n",
        "\n",
        "In this notebook we made the compatibity test to recreate te experiment of  https://arxiv.org/abs/1904.05255v1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Importing libraries for tensor manipulation and encoder testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qBzDzRDfh545"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, BertModel\n",
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Tensor manipulation operations with the relative position sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yOgiOGLizTz",
        "outputId": "670035dd-3b9c-47f0-a7e6-f4ffe6db8e8e"
      },
      "outputs": [],
      "source": [
        "bert_test_model = BertModel.from_pretrained('dccuchile/bert-base-spanish-wwm-cased', add_pooling_layer=False)\n",
        "tokenizer = AutoTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KHyVoBidi_hO"
      },
      "outputs": [],
      "source": [
        "text_sample = \" Siendo a todo ello testigos Juan Vel치zquez de Ortega, alguacil del Santo Oficio,  y Diego de Arganal y Juan de Vergara, vecinos y estantes en la dicha villa\"\n",
        "text_sample_masked = \" Siendo a todo ello testigos [PERSON], alguacil del [ORG],  y Diego de Arganal y Juan de Vergara, vecinos y estantes en la dicha villa\"\n",
        "text_sample_prepared = f'[CLS] {text_sample} [SEP] Juan Vel치zquez de Ortega [SEP] Santo Oficio [SEP]'\n",
        "text_sample_prepared_masked = f'[CLS] {text_sample_masked} [SEP] Juan Vel치zquez de Ortega [SEP] Santo Oficio [SEP]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[CLS]  Siendo a todo ello testigos [PERSON], alguacil del [ORG],  y Diego de Arganal y Juan de Vergara, vecinos y estantes en la dicha villa [SEP] Juan Vel치zquez de Ortega [SEP] Santo Oficio [SEP]'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_sample_prepared_masked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "frph73fOkRLA"
      },
      "outputs": [],
      "source": [
        "#masked_input_0 = tokenizer(text_sample_prepared, return_tensors='pt', add_special_tokens = False)\n",
        "masked_input_1 = tokenizer(text_sample_prepared_masked, return_tensors='pt', add_special_tokens = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPccBpBkkfuY",
        "outputId": "94d4031d-1dda-4345-c622-994e4fb34cbe"
      },
      "outputs": [],
      "source": [
        "#tokenizer.add_special_tokens(special_tokens_dict={})\n",
        "#TODO We need to add the special tokens to the tokenizer to have an specific embedding for them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zUWVR9uek3La"
      },
      "outputs": [],
      "source": [
        "# Here we obtain the logits from BERT encoder\n",
        "with torch.no_grad():\n",
        "\n",
        "    output_1 = bert_test_model(**masked_input_1) # type: ignore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsknmOG4k8vF",
        "outputId": "d7bf411e-1a04-4bed-9316-d011a30cfc20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "odict_keys(['last_hidden_state'])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_1.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 52, 768])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_1[0].shape # corresponds to (Batch, tokenized sequence lenght, last hidden states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AK42GjKclzY5"
      },
      "outputs": [],
      "source": [
        "\n",
        "cls_ = output_1[0][:, 0, :] #checking on how to extract the specific CLS hidden state for another type of experimentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here we develop the index technique to separate the EN1 [SEP] EN2 [SEP] part as in the article\n",
        "sep_token_id = tokenizer.sep_token_id\n",
        "number_of_sep = len((masked_input_1['input_ids']== sep_token_id).nonzero(as_tuple = True)[1]) # type: ignore # this returns a boolean vector with the positions of the tokens SEP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LfZNCZWxk_cH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"sep_index_0_0 = (masked_input_0['input_ids'] == sep_token_id).nonzero(as_tuple=True)[1][0].item()\\nsep_index_1_0 = (masked_input_1['input_ids'] == sep_token_id).nonzero(as_tuple=True)[1][0].item()\\nsep_index_0_1 = (masked_input_0['input_ids'] == sep_token_id).nonzero(as_tuple=True)[1][1].item()\\nsep_index_1_1 = (masked_input_1['input_ids'] == sep_token_id).nonzero(as_tuple=True)[1][1].item()\""
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO We need to rethink this part (IMPLEMENTED ON NOTEBOOK 0.1. Network_desing.ipynb)\n",
        "'''sep_index_0_0 = (masked_input_0['input_ids'] == sep_token_id).nonzero(as_tuple=True)[1][0].item()\n",
        "sep_index_1_0 = (masked_input_1['input_ids'] == sep_token_id).nonzero(as_tuple=True)[1][0].item()\n",
        "sep_index_0_1 = (masked_input_0['input_ids'] == sep_token_id).nonzero(as_tuple=True)[1][1].item()\n",
        "sep_index_1_1 = (masked_input_1['input_ids'] == sep_token_id).nonzero(as_tuple=True)[1][1].item()'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_go3-dEqjMK",
        "outputId": "a40a159e-5ebe-4707-8b71-80473aa97bdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'entiti_1_hidden_states_0.shape, entiti_2_hidden_states_0.shape, cls_token_0.shape'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'entiti_1_hidden_states_0.shape, entiti_2_hidden_states_0.shape, cls_token_0.shape'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0qWBU5M1uolL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'tensor_concatenated = torch.cat((cls_, entiti_1_hidden_states_0, entiti_2_hidden_states_0), dim=1)'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'tensor_concatenated = torch.cat((cls_, entiti_1_hidden_states_0, entiti_2_hidden_states_0), dim=1)'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYrCIWl6u65-",
        "outputId": "90226f8d-26a3-4f0b-df3b-2c782d5eb5b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'tensor_concatenated.shape'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'tensor_concatenated.shape'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3neRX6V1ygoZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'suqeezed_concat = tensor_concatenated.squeeze(0)'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'suqeezed_concat = tensor_concatenated.squeeze(0)'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ymjib87MylWQ",
        "outputId": "afe3ff7e-ec43-4d0f-a5b8-5955422235aa"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'suqeezed_concat' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msuqeezed_concat\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
            "\u001b[0;31mNameError\u001b[0m: name 'suqeezed_concat' is not defined"
          ]
        }
      ],
      "source": [
        "suqeezed_concat.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Creating the Relation extraction part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "V_1 = torch.randn(size=(8, 16, 768)) #simulating the hidden state of BERT (Batch size, sequence lenght, hidden_dimensions)\n",
        "\n",
        "V_2 = torch.randn(size= (8, 16, 20)) # Simulating the embedding of Relative position of ENT1 (Batch size, sequence lenght, hidden_dimensions)\n",
        "\n",
        "V_3 = torch.randn(size= (8, 16, 20)) # Simulating the embedding of Relative position of ENT2 (Batch size, sequence lenght, hidden_dimensions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "stack expects each tensor to be equal size, but got [8, 16, 768] at entry 0 and [8, 16, 20] at entry 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Stacked \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mV_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV_3\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# They could not use  stack\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [8, 16, 768] at entry 0 and [8, 16, 20] at entry 1"
          ]
        }
      ],
      "source": [
        "Stacked = torch.stack(tensors=(V_1, V_2, V_3), dim=0) # They could not use  stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (768) must match the size of tensor b (20) at non-singleton dimension 2",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m added \u001b[38;5;241m=\u001b[39m \u001b[43mV_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mV_2\u001b[49m \u001b[38;5;241m+\u001b[39m V_3 \u001b[38;5;66;03m# Neither element wise addition\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (768) must match the size of tensor b (20) at non-singleton dimension 2"
          ]
        }
      ],
      "source": [
        "added = V_1 + V_2 + V_3 # Neither element wise addition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 16, 2, 20])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Stacked_positions = torch.stack(tensors=(V_2, V_3), dim=2) # Nor stacking just the positions as it makes an illshaped tensor \n",
        "Stacked_positions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 16, 808])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "V_4 = torch.concat(tensors=(V_1, V_2, V_3), dim=2) # This is the correct formula\n",
        "V_4.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating the biLSTM as the article stated. We can try to with a GRU \n",
        "lstm = nn.LSTM(\n",
        "    input_size=V_1.shape[2] + V_2.shape[2] + V_3.shape[2], #Simulating the input size of the concatenated tensor\n",
        "    hidden_size=768, #Hidden size reported in the article\n",
        "    num_layers=1, #layers reported in the article\n",
        "    batch_first=True,\n",
        "    bidirectional=True # As the article suggests, the hidden size of every LSTM is 768, so the final hidden size of th BiLSTM is 768 *2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "gru = nn.GRU(\n",
        "    input_size=V_1.shape[2] + V_2.shape[2] + V_3.shape[2],\n",
        "    hidden_size=768,\n",
        "    num_layers=1,\n",
        "    batch_first=True,\n",
        "    bidirectional=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "hidd_state, cell_state = lstm(V_4)\n",
        "gru_hidd, gru_cell_state = gru(V_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tuple, torch.Tensor)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(cell_state), type(gru_cell_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 8, 768])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cell_state[0].shape #Final hidden state of the whole sequence (2 due to bidirectionality)\n",
        "cell_state[1].shape #Final cell state of the whole sequence (2 due to bidirectionality)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "concatenated_hidden = torch.concat(tensors=(cell_state[0][0], cell_state[0][1]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 1536])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_hidden.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 8, 768])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gru_cell_state.shape #Final hidden state of the whole sequence (2 due to bidirectionality)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([8, 16, 1536]), torch.Size([8, 16, 1536]))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hidd_state.shape, gru_hidd.shape # hidden states per element in the sequence. the ones we want to use to perfom the inference (?) we can prepare anothe model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 1536, 16])\n",
            "torch.Size([8, 1536, 1])\n",
            "torch.Size([8, 1536])\n"
          ]
        }
      ],
      "source": [
        "# As the MLP expects a Vector of (batch size, number of classes) we need to average the tensors to pass the Tokenclassification into SequenceClassification task.\n",
        "m = nn.AdaptiveAvgPool1d(1)\n",
        "hidden_permuted = hidd_state.permute(0,2,1) # this is necessary as Tensor.permute expects (Batch, Features, Sequence lenght)\n",
        "hidden_avg_pool = m(hidden_permuted)\n",
        "print(hidden_permuted.shape)\n",
        "print(hidden_avg_pool.shape)\n",
        "print(hidden_avg_pool.squeeze(2).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the MLP it is not defined, so we created one class with a GELU activation. Here we use final hidden states from LSTM/GRU\n",
        "class Classifier_MLP(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_dim,\n",
        "            hidden_dim, #It is said the dimensions are 300 in the article.\n",
        "            dropout_rate,\n",
        "            Gelu_aproximation,\n",
        "            output_dim\n",
        "            ):\n",
        "        super(Classifier_MLP, self).__init__()\n",
        "        self.MLP_block = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.GELU(approximate=Gelu_aproximation), # Most of the articles and new activations use Gelu as a better option than normal Relu. We choose based on this asumption\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(approximate=Gelu_aproximation),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        pooled_input = x\n",
        "        return self.MLP_block(pooled_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This one makes the pooling with per token hidden state.\n",
        "class Classifier_MLP_with_Pooling(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_dim,\n",
        "            hidden_dim, #It is said the dimensions are 300 in the article.\n",
        "            dropout_rate,\n",
        "            Gelu_aproximation,\n",
        "            output_dim\n",
        "            ):\n",
        "        super(Classifier_MLP_with_Pooling, self).__init__()\n",
        "        self.pooler = nn.AdaptiveAvgPool1d(1)\n",
        "        self.MLP_block = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.GELU(approximate=Gelu_aproximation), # Most of the articles and new activations use Gelu as a better option than normal Relu. We choose based on this asumption\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(approximate=Gelu_aproximation),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        pooled_input = self.pooler(x.permute(0,2,1))\n",
        "        pooled_input = pooled_input.squeeze(2)\n",
        "        return self.MLP_block(pooled_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlp = Classifier_MLP_with_Pooling(input_dim=1536, hidden_dim=300,dropout_rate=0.1,Gelu_aproximation='none', output_dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "out = mlp(hidd_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 10])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out.shape #The output shape is a pooled output of the original token classification sequence with (Batch_size and number of classes.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Final Network:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classifier with Pooling at the end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from typing import Optional, Union, Tuple\n",
        "from torch.nn import CrossEntropyLoss, MSELoss, BCEWithLogitsLoss\n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "from transformers.models.bert.modeling_bert import SequenceClassifierOutput\n",
        "\n",
        "\n",
        "class Classifier_MLP(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            dropout_rate,\n",
        "            Gelu_aproximation,\n",
        "            output_dim\n",
        "            ):\n",
        "        super(Classifier_MLP, self).__init__()\n",
        "        self.MLP_block = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.GELU(approximate=Gelu_aproximation), \n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(approximate=Gelu_aproximation),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        pooled_input = x\n",
        "        return self.MLP_block(pooled_input)\n",
        "class PooledSimpleBertLikeForSequenceClassification(BertPreTrainedModel):\n",
        "    def __init__(\n",
        "            self,\n",
        "            config,\n",
        "            Recurrent_net_config: str,\n",
        "            num_positional_embeddings: int,\n",
        "            positional_embedding_dim: int,\n",
        "            padding_idx: Optional[int],\n",
        "            MLP_hidden_size:int,\n",
        "            Gelu_aproximation: str \n",
        "            ):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.positional_embedding = nn.Embedding(\n",
        "            num_embeddings=num_positional_embeddings,\n",
        "            embedding_dim=positional_embedding_dim,\n",
        "            padding_idx=padding_idx\n",
        "            )\n",
        "        if isinstance(Recurrent_net_config, str):\n",
        "            if Recurrent_net_config == 'LSTM':\n",
        "                self.Bidirectional_block = nn.LSTM(\n",
        "                    input_size=self.config.hidden_size +(self.positional_embedding.embedding_dim *2),\n",
        "                    hidden_size=self.config.hidden_size,\n",
        "                    num_layers=1,\n",
        "                    batch_first=True,\n",
        "                    bidirectional=True\n",
        "                )\n",
        "            else:\n",
        "                Recurrent_net_config == 'GRU':\n",
        "                self.Bidirectional_block = nn.GRU(\n",
        "                    input_size=self.config.hidden_size +(self.positional_embedding.embedding_dim *2),\n",
        "                    hidden_size=self.config.hidden_size,\n",
        "                    num_layers=1,\n",
        "                    batch_first=True,\n",
        "                    bidirectional=True\n",
        "                )\n",
        "        else:\n",
        "            raise ValueError('The value must be \"LSTM\" or \"GRU\"')\n",
        "    \n",
        "        self.classifier = Classifier_MLP_with_Pooling(\n",
        "            input_dim=self.config.hidden_size*2,\n",
        "            hidden_dim= MLP_hidden_size,\n",
        "            dropout_rate=classifier_dropout,\n",
        "            Gelu_aproximation=Gelu_aproximation,\n",
        "            output_dim=self.num_labels\n",
        "        )\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
        "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
        "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        #TODO We need to specify the forward process and the maniputalation of the tensors.\n",
        "  \n",
        "        sep_token_id = 5\n",
        "        sep_indices = (input_ids == sep_token_id).nonzero(as_tuple=True)[1]\n",
        "        number_of_sep = len(sep_indices)\n",
        "\n",
        "        cls_output = outputs[0][:, 0, :]\n",
        "\n",
        "        if number_of_sep == 3:\n",
        "            sep_index_0 = sep_indices[0].item()\n",
        "            sep_index_1 = sep_indices[1].item()\n",
        "            sep_index_2 = sep_indices[2].item()\n",
        "            entiti_1_output = outputs[0][:, sep_index_0 + 1:sep_index_1, :]\n",
        "            entiti_2_output = outputs[0][:, sep_index_1 + 1:sep_index_2, :]\n",
        "            pooled_output = torch.cat((cls_output.unsqueeze(1), entiti_1_output, entiti_2_output), dim=1)\n",
        "        elif number_of_sep == 2:\n",
        "            sep_index_0 = sep_indices[0].item()\n",
        "            sep_index_1 = sep_indices[1].item()\n",
        "            entiti_1_output = outputs[0][:, sep_index_0 + 1:sep_index_1, :]\n",
        "            pooled_output = torch.cat((cls_output.unsqueeze(1), entiti_1_output), dim=1)\n",
        "        else:\n",
        "            raise ValueError(\"Input does not contain the required number of [SEP] tokens, which is 2 or 3.\")\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classifier with the final states of the LSTM/GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from typing import Optional, Union, Tuple\n",
        "from torch.nn import CrossEntropyLoss, MSELoss, BCEWithLogitsLoss\n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "from transformers.models.bert.modeling_bert import SequenceClassifierOutput\n",
        "\n",
        "\n",
        "class Classifier_MLP(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            dropout_rate,\n",
        "            Gelu_aproximation,\n",
        "            output_dim\n",
        "            ):\n",
        "        super(Classifier_MLP, self).__init__()\n",
        "        self.MLP_block = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.GELU(approximate=Gelu_aproximation), \n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(approximate=Gelu_aproximation),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        pooled_input = x\n",
        "        return self.MLP_block(pooled_input)\n",
        "class PooledSimpleBertLikeForSequenceClassification(BertPreTrainedModel):\n",
        "    def __init__(\n",
        "            self,\n",
        "            config,\n",
        "            Recurrent_net_config: str,\n",
        "            num_positional_embeddings: int,\n",
        "            positional_embedding_dim: int,\n",
        "            padding_idx: Optional[int],\n",
        "            MLP_hidden_size:int,\n",
        "            Gelu_aproximation: str \n",
        "            ):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.positional_embedding = nn.Embedding(\n",
        "            num_embeddings=num_positional_embeddings,\n",
        "            embedding_dim=positional_embedding_dim,\n",
        "            padding_idx=padding_idx\n",
        "            )\n",
        "        if isinstance(Recurrent_net_config, str):\n",
        "            if Recurrent_net_config == 'LSTM':\n",
        "                self.Bidirectional_block = nn.LSTM(\n",
        "                    input_size=self.config.hidden_size +(self.positional_embedding.embedding_dim *2),\n",
        "                    hidden_size=self.config.hidden_size,\n",
        "                    num_layers=1,\n",
        "                    batch_first=True,\n",
        "                    bidirectional=True\n",
        "                )\n",
        "            else:\n",
        "                Recurrent_net_config == 'GRU':\n",
        "                self.Bidirectional_block = nn.GRU(\n",
        "                    input_size=self.config.hidden_size +(self.positional_embedding.embedding_dim *2),\n",
        "                    hidden_size=self.config.hidden_size,\n",
        "                    num_layers=1,\n",
        "                    batch_first=True,\n",
        "                    bidirectional=True\n",
        "                )\n",
        "        else:\n",
        "            raise ValueError('The value must be \"LSTM\" or \"GRU\"')\n",
        "    \n",
        "        self.classifier = Classifier_MLP_with_Pooling(\n",
        "            input_dim=self.config.hidden_size*2,\n",
        "            hidden_dim= MLP_hidden_size,\n",
        "            dropout_rate=classifier_dropout,\n",
        "            Gelu_aproximation=Gelu_aproximation,\n",
        "            output_dim=self.num_labels\n",
        "        )\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
        "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
        "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        #TODO We need to specify the forward process and the maniputalation of the tensors.\n",
        "  \n",
        "        sep_token_id = 5\n",
        "        sep_indices = (input_ids == sep_token_id).nonzero(as_tuple=True)[1]\n",
        "        number_of_sep = len(sep_indices)\n",
        "\n",
        "        cls_output = outputs[0][:, 0, :]\n",
        "\n",
        "        if number_of_sep == 3:\n",
        "            sep_index_0 = sep_indices[0].item()\n",
        "            sep_index_1 = sep_indices[1].item()\n",
        "            sep_index_2 = sep_indices[2].item()\n",
        "            entiti_1_output = outputs[0][:, sep_index_0 + 1:sep_index_1, :]\n",
        "            entiti_2_output = outputs[0][:, sep_index_1 + 1:sep_index_2, :]\n",
        "            pooled_output = torch.cat((cls_output.unsqueeze(1), entiti_1_output, entiti_2_output), dim=1)\n",
        "        elif number_of_sep == 2:\n",
        "            sep_index_0 = sep_indices[0].item()\n",
        "            sep_index_1 = sep_indices[1].item()\n",
        "            entiti_1_output = outputs[0][:, sep_index_0 + 1:sep_index_1, :]\n",
        "            pooled_output = torch.cat((cls_output.unsqueeze(1), entiti_1_output), dim=1)\n",
        "        else:\n",
        "            raise ValueError(\"Input does not contain the required number of [SEP] tokens, which is 2 or 3.\")\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
