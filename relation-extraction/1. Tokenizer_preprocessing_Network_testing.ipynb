{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O. Importing libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from training_utils import tokenize_and_position_sequence\n",
    "from relation_extraction_encoders_modeling import Classifier_MLP, Classifier_MLP_with_AvgPooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/tensorboard/Documentos/1. D4R/Relation extraction/prompted_RE_senteces_19-06-24.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'r' ) as f:\n",
    "    data = json.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Adding the Special Tokens to the Model\n",
    "\n",
    "This part is quite tricky, so I will explain it step by step:\n",
    "\n",
    "1. First, load the normal tokenizer.\n",
    "\n",
    "2. Add the unused tokens. Include as many unused tokens as special tokens you intend to add (e.g., if you have 6 special tokens, add 6 unused tokens).\n",
    "\n",
    "3. Save the normal tokenizer with the new tokens to access the `vocab.txt` file. You will find [unused*] tokens listed there.\n",
    "\n",
    "4. Follow the instructions below, extracted from [this GitHub issue](https://github.com/huggingface/transformers/issues/27974):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    hey! You should modify manually both the **added_tokens_decoder field** (saved in the `tokenizer_config.json` ) and the **added_tokens field** (saved in the `tokenizer.json`). We don't really support this manually, but that is the recommended way to do it! (If the reserved tokens were already part of the vocab, so not AddedTokens, then you have to overwrite the vocab as well, the `vocab files`, to make sure they are removed) that would be hard than if it's just the content of the added tokens that you are trying to modify ðŸ˜‰\n",
    "\n",
    "\n",
    "\n",
    "This involves modifying both the `added_tokens_decoder` field in `tokenizer_config.json` and the `added_tokens` field in `tokenizer.json`. If the reserved tokens were already in the vocab and not AddedTokens, you may need to overwrite the vocab files to ensure they are correctly managed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step Zero\n",
    "tokenizer = AutoTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 1: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 4: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 5: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 6: AddedToken(\"[unused0]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 7: AddedToken(\"[unused1]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 8: AddedToken(\"[unused2]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 9: AddedToken(\"[unused3]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 10: AddedToken(\"[unused4]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1\n",
    "special_tokens_list =['[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]']\n",
    "# Add special tokens to the tokenizer\n",
    "tokenizer.add_special_tokens({'additional_special_tokens':special_tokens_list}) # type: ignore\n",
    "tokenizer.added_tokens_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/tensorboard/Documentos/1. D4R/Relation extraction/custom basic model/custom tokenizer/tokenizer_config.json',\n",
       " '/home/tensorboard/Documentos/1. D4R/Relation extraction/custom basic model/custom tokenizer/special_tokens_map.json',\n",
       " '/home/tensorboard/Documentos/1. D4R/Relation extraction/custom basic model/custom tokenizer/vocab.txt',\n",
       " '/home/tensorboard/Documentos/1. D4R/Relation extraction/custom basic model/custom tokenizer/added_tokens.json',\n",
       " '/home/tensorboard/Documentos/1. D4R/Relation extraction/custom basic model/custom tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2\n",
    "tokenizer.save_pretrained('/home/tensorboard/Documentos/1. D4R/Relation extraction/custom basic model/custom tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the step 3 is following the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 1: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 4: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 5: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 6: AddedToken(\"[PERSON]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 7: AddedToken(\"[PERSON_REFERENCE]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 8: AddedToken(\"[ORG]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 9: AddedToken(\"[PLACE]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 10: AddedToken(\"[CONTEXT]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If everything goes well, executing this line of code should solve it\n",
    "path_of_custom_tokenizer = '/home/tensorboard/Documentos/1. D4R/Relation extraction/custom basic model/custom tokenizer'\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_of_custom_tokenizer)\n",
    "tokenizer.added_tokens_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31002, 31002)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer), tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenizing inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_ids = {'Actos Procesales':0,'Circunstancial': 1, 'Connivencia': 2, 'Materia TeolÃ³gica': 3, 'Pertenencia': 4, 'Roles Procesales': 5}\n",
    "ids_to_labels = {v:k for k, v in labels_to_ids.items()}\n",
    "labels_list = [x for x in ids_to_labels.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompted_sentence = [i['Prompted_sentence'] for i in data]\n",
    "labels = [i['Label'] for i in data]\n",
    "encoded_labels = [[k] for va in labels for k, v in ids_to_labels.items() if va == v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels), len(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenize_and_position_sequence(sequences=prompted_sentence, tokenizer=tokenizer, labels= encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs['input_ids'].shape, tokenized_inputs['labels'].shape, tokenized_inputs['sequence_positions'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Checking the model can handle the computation with Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = tokenized_inputs = tokenize_and_position_sequence(sequences=prompted_sentence[100], tokenizer=tokenizer, labels=encoded_labels[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = bert_test_model(text_sample['input_ids'],\n",
    "            attention_mask=text_sample.attention_mask,\n",
    "            token_type_ids=text_sample.token_type_ids) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0].shape, text_sample['sequence_positions'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_token_id = 5\n",
    "sep_indices = (text_sample['input_ids'] == sep_token_id).nonzero(as_tuple=True)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_tensor = outputs[0][:,:sep_indices+1]\n",
    "rest_positions = torch.stack([text_sample['sequence_positions'][:,0,:sep_indices+1], text_sample['sequence_positions'][:,1,:sep_indices+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_positions.shape, rest_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_embedding = nn.Embedding(\n",
    "    num_embeddings=500,\n",
    "    embedding_dim=20,\n",
    "    padding_idx=499\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_positions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_1 = position_embedding(rest_positions[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_2 = position_embedding(rest_positions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_tensor.shape, post_1.shape, post_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tensor = torch.cat(tensors=(rest_tensor, post_1, post_2), dim=-1)\n",
    "full_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_tensor.shape[2] +(post_1.shape[2]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_stack =nn.LSTM(\n",
    "                    input_size=rest_tensor.shape[2] +(post_1.shape[2]*2),\n",
    "                    hidden_size=rest_tensor.shape[2],\n",
    "                    num_layers=1,\n",
    "                    batch_first=True,\n",
    "                    bidirectional=True\n",
    "                )\n",
    "\n",
    "bidirectional_stack_GRU =nn.GRU(\n",
    "                    input_size=rest_tensor.shape[2] +(post_1.shape[2]*2),\n",
    "                    hidden_size=rest_tensor.shape[2],\n",
    "                    num_layers=1,\n",
    "                    batch_first=True,\n",
    "                    bidirectional=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_hidden_states, final_hidden_state = bidirectional_stack(full_tensor)\n",
    "token_hidden_states_GRU, final_hidden_state_GRU = bidirectional_stack_GRU(full_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_hidden_states.shape, final_hidden_state[0].shape, token_hidden_states_GRU.shape, final_hidden_state_GRU.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hiddenconcat = torch.cat(tensors=(final_hidden_state[0][0], final_hidden_state[1][1]), dim=-1)\n",
    "final_hiddenconcat_GRU = torch.cat(tensors=(final_hidden_state_GRU[0], final_hidden_state_GRU[1]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hiddenconcat.shape, final_hiddenconcat_GRU.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_hidden_state = Classifier_MLP(\n",
    "    input_dim=768*2,\n",
    "    hidden_dim=300,\n",
    "    output_dim=1,\n",
    "    dropout_rate=0.5,\n",
    "    Gelu_aproximation='none'\n",
    ")\n",
    "\n",
    "classificer_pooling = Classifier_MLP_with_Pooling(\n",
    "    input_dim=768*2,\n",
    "    hidden_dim=300,\n",
    "    output_dim=1,\n",
    "    dropout_rate=0.5,\n",
    "    Gelu_aproximation='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_final = classifier_hidden_state(final_hiddenconcat)\n",
    "out_pooling = classificer_pooling(token_hidden_states)\n",
    "out_final_GRU = classifier_hidden_state(final_hiddenconcat_GRU)\n",
    "out_pooling_GRU = classificer_pooling(token_hidden_states_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_final.shape, out_pooling.shape, out_final_GRU.shape, out_pooling_GRU.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
