{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing of te networks desing and training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. importing and preprocessing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import(\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments)\n",
    "from training_utils import tokenize_and_position_sequence, baseDataset\n",
    "from relation_extraction_encoders_modeling import Classifier_MLP\n",
    "from torchinfo import summary\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/tensorboard/Documentos/1. D4R/Relation extraction/test_20-06-24.csv'\n",
    "custom_model_path = '/home/tensorboard/Documentos/1. D4R/Relation extraction/custom basic model/basic_model'\n",
    "custom_tokenizer_path = '/home/tensorboard/Documentos/1. D4R/Relation extraction/custom basic model/custom tokenizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Doña Ana Enríquez y de oídas lo depuso en 23...</td>\n",
       "      <td>Actos Procesales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Doña Juana de Fonseca lo depuso en 19 de abr...</td>\n",
       "      <td>Actos Procesales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Fray Alonso de Horozco depone de oídas ,  lo...</td>\n",
       "      <td>Actos Procesales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Testigoº5 Doña Antonia de Branches depuso a ...</td>\n",
       "      <td>Actos Procesales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Testigoº No declarado en el proceso Fray Ant...</td>\n",
       "      <td>Actos Procesales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>1227</td>\n",
       "      <td>Siendo a todo ello testigos Juan Velázquez d...</td>\n",
       "      <td>Roles Procesales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>1228</td>\n",
       "      <td>Siendo a todo ello testigos Juan Velázquez d...</td>\n",
       "      <td>Roles Procesales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>1229</td>\n",
       "      <td>Siendo a todo ello testigos Juan Velázquez d...</td>\n",
       "      <td>Roles Procesales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>1230</td>\n",
       "      <td>Siendo a todo ello testigos Juan Velázquez d...</td>\n",
       "      <td>Roles Procesales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>1231</td>\n",
       "      <td>Siendo a todo ello testigos Juan Velázquez d...</td>\n",
       "      <td>Roles Procesales</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1231 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                           Sentence  \\\n",
       "0              0    Doña Ana Enríquez y de oídas lo depuso en 23...   \n",
       "1              1    Doña Juana de Fonseca lo depuso en 19 de abr...   \n",
       "2              2    Fray Alonso de Horozco depone de oídas ,  lo...   \n",
       "3              3    Testigoº5 Doña Antonia de Branches depuso a ...   \n",
       "4              4    Testigoº No declarado en el proceso Fray Ant...   \n",
       "...          ...                                                ...   \n",
       "1226        1227    Siendo a todo ello testigos Juan Velázquez d...   \n",
       "1227        1228    Siendo a todo ello testigos Juan Velázquez d...   \n",
       "1228        1229    Siendo a todo ello testigos Juan Velázquez d...   \n",
       "1229        1230    Siendo a todo ello testigos Juan Velázquez d...   \n",
       "1230        1231    Siendo a todo ello testigos Juan Velázquez d...   \n",
       "\n",
       "                 Label  \n",
       "0     Actos Procesales  \n",
       "1     Actos Procesales  \n",
       "2     Actos Procesales  \n",
       "3     Actos Procesales  \n",
       "4     Actos Procesales  \n",
       "...                ...  \n",
       "1226  Roles Procesales  \n",
       "1227  Roles Procesales  \n",
       "1228  Roles Procesales  \n",
       "1229  Roles Procesales  \n",
       "1230  Roles Procesales  \n",
       "\n",
       "[1231 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_ids = {'Actos Procesales':0,'Circunstancial': 1, 'Connivencia': 2, 'Materia Teológica': 3, 'Pertenencia': 4, 'Roles Procesales': 5}\n",
    "ids_to_labels = {v:k for k, v in labels_to_ids.items()}\n",
    "labels_list = [x for x in ids_to_labels.values()]\n",
    "prompted_sentence = [i for i in data['Sentence']]\n",
    "labels = [i for i in data['Label']]\n",
    "encoded_labels = [[k] for va in labels for k, v in ids_to_labels.items() if va == v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated tokenizer vocabulary size: 31002 and the len of the tokenizer is 31002\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(custom_tokenizer_path)\n",
    "vocab_size = tokenizer.vocab_size \n",
    "print(f\"Updated tokenizer vocabulary size: {vocab_size} and the len of the tokenizer is {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 1: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 4: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 5: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 6: AddedToken(\"[PERSON]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 7: AddedToken(\"[PERSON_REFERENCE]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 8: AddedToken(\"[ORG]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 9: AddedToken(\"[PLACE]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 10: AddedToken(\"[CONTEXT]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.added_tokens_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenizer(\n",
    "            prompted_sentence,\n",
    "            truncation=True,\n",
    "\n",
    "            padding=True,\n",
    "\n",
    "            return_tensors='pt'\n",
    "        ) # \n",
    "tokenized_inputs['labels'] = torch.tensor(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tensors have positive values for the position embedding layer\n"
     ]
    }
   ],
   "source": [
    "for i, tensor in enumerate(tokenized_inputs['sequence_positions'][:,0]):\n",
    "    if min(tensor) < 0 or min(tensor) == 0:\n",
    "        print(f'index: {i}, max: {max(tensor)}, min: {min(tensor)}')\n",
    "else: \n",
    "    print('all tensors have positive values for the position embedding layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tensors have positive values for the position embedding layer\n"
     ]
    }
   ],
   "source": [
    "for i, tensor in enumerate(tokenized_inputs['sequence_positions'][:,1]):\n",
    "    if min(tensor) < 0 or min(tensor) == 0:\n",
    "        print(f'index: {i}, max: {max(tensor)}, min: {min(tensor)}')\n",
    "else: \n",
    "    print('all tensors have positive values for the position embedding layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from typing import Optional, Union, Tuple\n",
    "from torch.nn import CrossEntropyLoss, MSELoss, BCEWithLogitsLoss\n",
    "from transformers import BertPreTrainedModel, BertModel, BertForSequenceClassification\n",
    "from transformers.models.bert.modeling_bert import SequenceClassifierOutput\n",
    "from relation_extraction_encoders_modeling import Classifier_MLP_with_AvgPooling, Classifier_MLP\n",
    "class SimpleBertLikeForSequenceClassificationWithLSTM(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Simple BERT-like model with LSTM for sequence classification.\n",
    "\n",
    "    This model integrates BERT embeddings with a bidirectional LSTM layer\n",
    "    followed by an MLP classifier for sequence classification tasks.\n",
    "\n",
    "    Parameters:\n",
    "        config (:obj:`BertConfig`): Configuration class for BERT model.\n",
    "        num_positional_embeddings (int): Number of positional embeddings.\n",
    "        positional_embedding_dim (int): Dimensionality of positional embeddings.\n",
    "        padding_idx (Optional[int]): Index for padding tokens in positional embeddings.\n",
    "        MLP_hidden_size (int): Hidden size of the MLP classifier.\n",
    "        Gelu_aproximation (str): Approximation method for GELU activation function.\n",
    "\n",
    "    Attributes:\n",
    "        num_labels (int): Number of labels for sequence classification.\n",
    "        config (:obj:`BertConfig`): Configuration class instance for BERT model.\n",
    "        bert (:obj:`BertModel`): BERT model without pooling layer.\n",
    "        positional_embedding (:obj:`nn.Embedding`): Positional embedding layer.\n",
    "        Bidirectional_block (:obj:`nn.LSTM`): Bidirectional LSTM layer.\n",
    "        classifier (:obj:`Classifier_MLP`): MLP classifier for final prediction.\n",
    "\n",
    "    Methods:\n",
    "        forward(input_ids, attention_mask, token_type_ids, position_ids, head_mask,\n",
    "                inputs_embeds, labels, output_attentions, output_hidden_states,\n",
    "                return_dict, sequence_positions):\n",
    "            Performs forward pass through the model and returns logits or SequenceClassifierOutput.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            config,\n",
    "            MLP_hidden_size: int = 384,\n",
    "            Gelu_aproximation: str = 'none'\n",
    "            ):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=True)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = Classifier_MLP(\n",
    "            input_dim=self.config.hidden_size,\n",
    "            hidden_dim=MLP_hidden_size,\n",
    "            dropout_rate=classifier_dropout,\n",
    "            Gelu_aproximation=Gelu_aproximation,\n",
    "            output_dim=self.num_labels\n",
    "        )\n",
    " \n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            token_type_ids: Optional[torch.Tensor] = None,\n",
    "            position_ids: Optional[torch.Tensor] = None,\n",
    "            head_mask: Optional[torch.Tensor] = None,\n",
    "            inputs_embeds: Optional[torch.Tensor] = None,\n",
    "            labels: Optional[torch.Tensor] = None,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "\n",
    "        ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "\n",
    "\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "   \n",
    "        # Extract indices for separation tokens (assuming sep_token_id is 5)\n",
    "    \n",
    "        # Separate BERT output into segments\n",
    "        pooled_output= outputs[1]\n",
    "   \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        # Check logits for NaNs\n",
    "\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "\n",
    "\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/tensorboard/Documentos/1. D4R/Relation extraction/custom basic model/basic_model and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "test_model = BertForSequenceClassification.from_pretrained(\n",
    "                                                                            custom_model_path,\n",
    "                                                                            num_labels= len(labels_list),\n",
    "                                                                            id2label=ids_to_labels,\n",
    "                                                                            label2id=labels_to_ids\n",
    "                                                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input_ids = tokenized_inputs['input_ids'][0:10] # Adds batch dimension\n",
    "    attention_mask = tokenized_inputs['attention_mask'][0:10]\n",
    "    token_type_ids = tokenized_inputs['token_type_ids'][0:10]\n",
    "\n",
    "    \n",
    "    labels = tokenized_inputs['labels'][0:10]\n",
    "    \n",
    "    out = test_model(input_ids=input_ids,\n",
    "                     attention_mask=attention_mask,\n",
    "                     token_type_ids=token_type_ids,\n",
    "\n",
    "                     labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(1.7308), logits=tensor([[ 0.1731,  0.2718,  0.1162, -0.1931, -0.0942,  0.1442],\n",
       "        [ 0.2471,  0.1847,  0.0234, -0.1588,  0.0077,  0.1365],\n",
       "        [ 0.3040,  0.3009,  0.0870, -0.1940,  0.0272,  0.1686],\n",
       "        [ 0.2933,  0.1098, -0.0486, -0.1002,  0.0377,  0.0655],\n",
       "        [ 0.2259,  0.1885,  0.0542, -0.1926,  0.0890,  0.1612],\n",
       "        [ 0.2196,  0.4372,  0.0671, -0.1535, -0.1551,  0.2606],\n",
       "        [ 0.0786,  0.2172, -0.0486, -0.0246, -0.0558,  0.2582],\n",
       "        [ 0.0786,  0.2172, -0.0486, -0.0246, -0.0558,  0.2582],\n",
       "        [ 0.0786,  0.2172, -0.0486, -0.0246, -0.0558,  0.2582],\n",
       "        [ 0.0786,  0.2172, -0.0486, -0.0246, -0.0558,  0.2582]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General variables:\n",
    "\n",
    "\n",
    "\n",
    "MODEL_SUMMARY_NAME = 'BETO'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LEARNING_RATE = 2e-4\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 4\n",
    "\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "WEIGHT_DECAY = 0.1\n",
    "EVALUATION_STRATEGY = \"steps\"\n",
    "LOGGING_STEPS = 10\n",
    "SAVE_STRATEGY = \"steps\"\n",
    "SAVE_STEPS = 500\n",
    "SAVE_ONLY_MODEL = True\n",
    "OUTPUT_DIRECTORY = f\"/home/tensorboard/Documentos/1. D4R/8. Models/RE-LSTM-{str(MODEL_SUMMARY_NAME)}-{str(LEARNING_RATE)}-lr-{str(NUM_TRAIN_EPOCHS)}-epochs\"\n",
    "LR_SCHEDULER_TYPE = 'constant'\n",
    "#['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup', 'inverse_sqrt', 'reduce_lr_on_plateau', 'cosine_with_min_lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load the evaluation metrics\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    precision_result = precision_metric.compute(predictions=predictions, references=labels)\n",
    "    recall_result = recall_metric.compute(predictions=predictions, references=labels)\n",
    "    f1_result = f1_metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision_result[\"precision\"],\n",
    "        \"recall\": recall_result[\"recall\"],\n",
    "        \"f1\": f1_result[\"f1\"]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacollator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "dataset = baseDataset(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Process\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=r'/home/tensorboard/Documentos/1. D4R/8. Models',\n",
    "    learning_rate=1e-4 ,\n",
    "    per_device_train_batch_size=10,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='steps',\n",
    "    save_only_model= True,\n",
    "    lr_scheduler_type='constant',\n",
    "    logging_steps= 10,\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=test_model, # type: ignore\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=datacollator,\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db52329d232b407ebddc138e76fc8a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9727, 'grad_norm': 27.092010498046875, 'learning_rate': 0.0001, 'epoch': 0.08}\n",
      "{'loss': 1.3893, 'grad_norm': 5.417557239532471, 'learning_rate': 0.0001, 'epoch': 0.16}\n",
      "{'loss': 1.2108, 'grad_norm': 5.926936149597168, 'learning_rate': 0.0001, 'epoch': 0.24}\n",
      "{'loss': 1.0682, 'grad_norm': 5.868988037109375, 'learning_rate': 0.0001, 'epoch': 0.32}\n",
      "{'loss': 0.7055, 'grad_norm': 5.5226874351501465, 'learning_rate': 0.0001, 'epoch': 0.4}\n",
      "{'loss': 1.051, 'grad_norm': 5.881610870361328, 'learning_rate': 0.0001, 'epoch': 0.48}\n",
      "{'loss': 0.8385, 'grad_norm': 2.8503479957580566, 'learning_rate': 0.0001, 'epoch': 0.56}\n",
      "{'loss': 0.9384, 'grad_norm': 3.5090768337249756, 'learning_rate': 0.0001, 'epoch': 0.65}\n",
      "{'loss': 0.7734, 'grad_norm': 2.2956290245056152, 'learning_rate': 0.0001, 'epoch': 0.73}\n",
      "{'loss': 0.7434, 'grad_norm': 5.564837455749512, 'learning_rate': 0.0001, 'epoch': 0.81}\n",
      "{'loss': 0.8529, 'grad_norm': 8.871514320373535, 'learning_rate': 0.0001, 'epoch': 0.89}\n",
      "{'loss': 0.8868, 'grad_norm': 52.10535430908203, 'learning_rate': 0.0001, 'epoch': 0.97}\n",
      "{'loss': 0.6771, 'grad_norm': 1.891082525253296, 'learning_rate': 0.0001, 'epoch': 1.05}\n",
      "{'loss': 0.9176, 'grad_norm': 6.107271194458008, 'learning_rate': 0.0001, 'epoch': 1.13}\n",
      "{'loss': 0.8989, 'grad_norm': 2.932620048522949, 'learning_rate': 0.0001, 'epoch': 1.21}\n",
      "{'loss': 0.7884, 'grad_norm': 9.488382339477539, 'learning_rate': 0.0001, 'epoch': 1.29}\n",
      "{'loss': 1.1304, 'grad_norm': 26.285551071166992, 'learning_rate': 0.0001, 'epoch': 1.37}\n",
      "{'loss': 1.0319, 'grad_norm': 4.805391788482666, 'learning_rate': 0.0001, 'epoch': 1.45}\n",
      "{'loss': 0.7711, 'grad_norm': 1.6635903120040894, 'learning_rate': 0.0001, 'epoch': 1.53}\n",
      "{'loss': 0.7753, 'grad_norm': 2.4645416736602783, 'learning_rate': 0.0001, 'epoch': 1.61}\n",
      "{'loss': 0.9129, 'grad_norm': 5.173066139221191, 'learning_rate': 0.0001, 'epoch': 1.69}\n",
      "{'loss': 0.8237, 'grad_norm': 3.282388687133789, 'learning_rate': 0.0001, 'epoch': 1.77}\n",
      "{'loss': 0.814, 'grad_norm': 9.164557456970215, 'learning_rate': 0.0001, 'epoch': 1.85}\n",
      "{'loss': 1.6031, 'grad_norm': 32.056007385253906, 'learning_rate': 0.0001, 'epoch': 1.94}\n",
      "{'loss': 0.988, 'grad_norm': 6.1541666984558105, 'learning_rate': 0.0001, 'epoch': 2.02}\n",
      "{'loss': 0.8683, 'grad_norm': 2.516355276107788, 'learning_rate': 0.0001, 'epoch': 2.1}\n",
      "{'loss': 0.704, 'grad_norm': 2.860875368118286, 'learning_rate': 0.0001, 'epoch': 2.18}\n",
      "{'loss': 0.7265, 'grad_norm': 2.5615084171295166, 'learning_rate': 0.0001, 'epoch': 2.26}\n",
      "{'loss': 0.6276, 'grad_norm': 11.867218017578125, 'learning_rate': 0.0001, 'epoch': 2.34}\n",
      "{'loss': 0.7965, 'grad_norm': 3.2975711822509766, 'learning_rate': 0.0001, 'epoch': 2.42}\n",
      "{'loss': 0.9487, 'grad_norm': 26.918481826782227, 'learning_rate': 0.0001, 'epoch': 2.5}\n",
      "{'loss': 0.8416, 'grad_norm': 4.755822658538818, 'learning_rate': 0.0001, 'epoch': 2.58}\n",
      "{'loss': 0.9175, 'grad_norm': 3.0138163566589355, 'learning_rate': 0.0001, 'epoch': 2.66}\n",
      "{'loss': 0.8614, 'grad_norm': 2.795074224472046, 'learning_rate': 0.0001, 'epoch': 2.74}\n",
      "{'loss': 0.9161, 'grad_norm': 3.4823904037475586, 'learning_rate': 0.0001, 'epoch': 2.82}\n",
      "{'loss': 0.8871, 'grad_norm': 54.6731071472168, 'learning_rate': 0.0001, 'epoch': 2.9}\n",
      "{'loss': 0.8725, 'grad_norm': 162.11619567871094, 'learning_rate': 0.0001, 'epoch': 2.98}\n",
      "{'loss': 1.0856, 'grad_norm': 2.9173948764801025, 'learning_rate': 0.0001, 'epoch': 3.06}\n",
      "{'loss': 0.8227, 'grad_norm': 7.1961493492126465, 'learning_rate': 0.0001, 'epoch': 3.15}\n",
      "{'loss': 1.0021, 'grad_norm': 5.905858516693115, 'learning_rate': 0.0001, 'epoch': 3.23}\n",
      "{'loss': 0.9432, 'grad_norm': 5.022397994995117, 'learning_rate': 0.0001, 'epoch': 3.31}\n",
      "{'loss': 1.1228, 'grad_norm': 4.083030700683594, 'learning_rate': 0.0001, 'epoch': 3.39}\n",
      "{'loss': 0.8123, 'grad_norm': 5.528926849365234, 'learning_rate': 0.0001, 'epoch': 3.47}\n",
      "{'loss': 0.982, 'grad_norm': 4.430200576782227, 'learning_rate': 0.0001, 'epoch': 3.55}\n",
      "{'loss': 1.1638, 'grad_norm': 3.9777557849884033, 'learning_rate': 0.0001, 'epoch': 3.63}\n",
      "{'loss': 0.8442, 'grad_norm': 6.182152271270752, 'learning_rate': 0.0001, 'epoch': 3.71}\n",
      "{'loss': 0.9971, 'grad_norm': 2.6354477405548096, 'learning_rate': 0.0001, 'epoch': 3.79}\n",
      "{'loss': 0.969, 'grad_norm': 6.606164455413818, 'learning_rate': 0.0001, 'epoch': 3.87}\n",
      "{'loss': 1.0419, 'grad_norm': 1.6771594285964966, 'learning_rate': 0.0001, 'epoch': 3.95}\n",
      "{'loss': 0.9737, 'grad_norm': 14.396988868713379, 'learning_rate': 0.0001, 'epoch': 4.03}\n",
      "{'loss': 0.8903, 'grad_norm': 3.716063976287842, 'learning_rate': 0.0001, 'epoch': 4.11}\n",
      "{'loss': 0.7404, 'grad_norm': 2.888951539993286, 'learning_rate': 0.0001, 'epoch': 4.19}\n",
      "{'loss': 1.0124, 'grad_norm': 3.02378249168396, 'learning_rate': 0.0001, 'epoch': 4.27}\n",
      "{'loss': 1.0763, 'grad_norm': 4.147278785705566, 'learning_rate': 0.0001, 'epoch': 4.35}\n",
      "{'loss': 0.9402, 'grad_norm': 40.92396926879883, 'learning_rate': 0.0001, 'epoch': 4.44}\n",
      "{'loss': 1.2172, 'grad_norm': 3.034477472305298, 'learning_rate': 0.0001, 'epoch': 4.52}\n",
      "{'loss': 1.0598, 'grad_norm': 4.176498889923096, 'learning_rate': 0.0001, 'epoch': 4.6}\n",
      "{'loss': 1.168, 'grad_norm': 3.433722972869873, 'learning_rate': 0.0001, 'epoch': 4.68}\n",
      "{'loss': 0.9444, 'grad_norm': 14.069852828979492, 'learning_rate': 0.0001, 'epoch': 4.76}\n",
      "{'loss': 1.146, 'grad_norm': 48.50559616088867, 'learning_rate': 0.0001, 'epoch': 4.84}\n",
      "{'loss': 0.9927, 'grad_norm': 3.7133095264434814, 'learning_rate': 0.0001, 'epoch': 4.92}\n",
      "{'loss': 0.9497, 'grad_norm': 1.0129374265670776, 'learning_rate': 0.0001, 'epoch': 5.0}\n",
      "{'loss': 0.8959, 'grad_norm': 123.25797271728516, 'learning_rate': 0.0001, 'epoch': 5.08}\n",
      "{'loss': 0.8452, 'grad_norm': 3.5247442722320557, 'learning_rate': 0.0001, 'epoch': 5.16}\n",
      "{'loss': 0.9468, 'grad_norm': 4.004998683929443, 'learning_rate': 0.0001, 'epoch': 5.24}\n",
      "{'loss': 0.8739, 'grad_norm': 4.994053840637207, 'learning_rate': 0.0001, 'epoch': 5.32}\n",
      "{'loss': 1.0777, 'grad_norm': 29.166040420532227, 'learning_rate': 0.0001, 'epoch': 5.4}\n",
      "{'loss': 0.7446, 'grad_norm': 2.8519091606140137, 'learning_rate': 0.0001, 'epoch': 5.48}\n",
      "{'loss': 1.057, 'grad_norm': 4.228240489959717, 'learning_rate': 0.0001, 'epoch': 5.56}\n",
      "{'loss': 0.9131, 'grad_norm': 2.35367488861084, 'learning_rate': 0.0001, 'epoch': 5.65}\n",
      "{'loss': 1.0508, 'grad_norm': 4.248357772827148, 'learning_rate': 0.0001, 'epoch': 5.73}\n",
      "{'loss': 0.8993, 'grad_norm': 4.906369686126709, 'learning_rate': 0.0001, 'epoch': 5.81}\n",
      "{'loss': 0.909, 'grad_norm': 2.421653985977173, 'learning_rate': 0.0001, 'epoch': 5.89}\n",
      "{'loss': 0.9857, 'grad_norm': 4.718787670135498, 'learning_rate': 0.0001, 'epoch': 5.97}\n",
      "{'loss': 0.8035, 'grad_norm': 16.588836669921875, 'learning_rate': 0.0001, 'epoch': 6.05}\n",
      "{'loss': 0.9093, 'grad_norm': 4.619691848754883, 'learning_rate': 0.0001, 'epoch': 6.13}\n",
      "{'loss': 0.8002, 'grad_norm': 2.9854588508605957, 'learning_rate': 0.0001, 'epoch': 6.21}\n",
      "{'loss': 0.7413, 'grad_norm': 3.519864320755005, 'learning_rate': 0.0001, 'epoch': 6.29}\n",
      "{'loss': 0.8813, 'grad_norm': 2.4292752742767334, 'learning_rate': 0.0001, 'epoch': 6.37}\n",
      "{'loss': 0.8991, 'grad_norm': 2.9488606452941895, 'learning_rate': 0.0001, 'epoch': 6.45}\n",
      "{'loss': 0.8442, 'grad_norm': 1.8575798273086548, 'learning_rate': 0.0001, 'epoch': 6.53}\n",
      "{'loss': 0.7763, 'grad_norm': 3.474846601486206, 'learning_rate': 0.0001, 'epoch': 6.61}\n",
      "{'loss': 1.0413, 'grad_norm': 3.8528544902801514, 'learning_rate': 0.0001, 'epoch': 6.69}\n",
      "{'loss': 0.8783, 'grad_norm': 2.5557894706726074, 'learning_rate': 0.0001, 'epoch': 6.77}\n",
      "{'loss': 0.8364, 'grad_norm': 2.838139533996582, 'learning_rate': 0.0001, 'epoch': 6.85}\n",
      "{'loss': 0.8827, 'grad_norm': 9.475300788879395, 'learning_rate': 0.0001, 'epoch': 6.94}\n",
      "{'loss': 0.8834, 'grad_norm': 3.613893985748291, 'learning_rate': 0.0001, 'epoch': 7.02}\n",
      "{'loss': 1.0305, 'grad_norm': 5.574263572692871, 'learning_rate': 0.0001, 'epoch': 7.1}\n",
      "{'loss': 1.1479, 'grad_norm': 2.927096366882324, 'learning_rate': 0.0001, 'epoch': 7.18}\n",
      "{'loss': 0.9507, 'grad_norm': 6.678187370300293, 'learning_rate': 0.0001, 'epoch': 7.26}\n",
      "{'loss': 1.1421, 'grad_norm': 4.448348045349121, 'learning_rate': 0.0001, 'epoch': 7.34}\n",
      "{'loss': 1.103, 'grad_norm': 5.557570934295654, 'learning_rate': 0.0001, 'epoch': 7.42}\n",
      "{'loss': 0.8989, 'grad_norm': 4.426867485046387, 'learning_rate': 0.0001, 'epoch': 7.5}\n",
      "{'loss': 1.1304, 'grad_norm': 5.297362327575684, 'learning_rate': 0.0001, 'epoch': 7.58}\n",
      "{'loss': 1.0629, 'grad_norm': 7.188180446624756, 'learning_rate': 0.0001, 'epoch': 7.66}\n",
      "{'loss': 1.2361, 'grad_norm': 4.813969135284424, 'learning_rate': 0.0001, 'epoch': 7.74}\n",
      "{'loss': 1.5112, 'grad_norm': 7.055257320404053, 'learning_rate': 0.0001, 'epoch': 7.82}\n",
      "{'loss': 1.43, 'grad_norm': 3.799448251724243, 'learning_rate': 0.0001, 'epoch': 7.9}\n",
      "{'loss': 1.5302, 'grad_norm': 4.620727062225342, 'learning_rate': 0.0001, 'epoch': 7.98}\n",
      "{'loss': 1.4406, 'grad_norm': 7.6192803382873535, 'learning_rate': 0.0001, 'epoch': 8.06}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/1. D4R/.venv/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/1. D4R/.venv/lib/python3.10/site-packages/transformers/trainer.py:2221\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
